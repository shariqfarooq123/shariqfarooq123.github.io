[
    {
        "title": "Adabins: Depth estimation using adaptive bins",
        "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "date": "2021",
        "abstract": "We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model.",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_AdaBins_Depth_Estimation_Using_Adaptive_Bins_CVPR_2021_paper.pdf",
        "code": "https://github.com/shariqfarooq123/AdaBins",
        "authors": ["SBhat", "IAlhashim", "PWonka"],
        "image": "/files/adaptive_bins.gif",
        "video": "https://youtube.com/watch?v=FmRjtw_UCnY&feature=shares"
    },
    {
        "title": "LocalBins: Improving Depth Estimation by Learning Local Distributions",
        "venue": "European Conference on Computer Vision (ECCV)",
        "date": "2022",
        "abstract": "We propose a novel architecture for depth estimation from a single image. The architecture itself is based on the popular encoder-decoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available.",
        "pdf": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610473.pdf",
        "code": "https://github.com/shariqfarooq123/LocalBins",
        "authors": ["SBhat", "IAlhashim", "PWonka"],
        "image": "/files/localbins.gif",
        "video": "https://youtube.com/watch?v=-p4pPFgNu-M&feature=shares"
    },
    {
        "title": "Self-Supervised Learning of Domain Invariant Features for Depth Estimation",
        "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
        "date": "2022",
        "abstract": "We tackle the problem of unsupervised synthetic-to-real domain adaptation for single image depth estimation. An essential building block of single image depth estimation is an encoder-decoder task network that takes RGB images as input and produces depth maps as output. In this paper, we propose a novel training strategy to force the task network to learn domain invariant representations in a self-supervised manner. Specifically, we extend self-supervised learning from traditional representation learning, which works on images from a single domain, to domain invariant representation learning, which works on images from two different domains by utilizing an image-to-image translation network. Firstly, we use an image-to-image translation network to transfer domain-specific styles between synthetic and real domains. This style transfer operation allows us to obtain similar images from the different domains. Secondly, we jointly train our task network and Siamese network with the same images from the different domains to obtain domain invariance for the task network. Finally, we fine-tune the task network using labeled synthetic and unlabeled real-world data. Our training strategy yields improved generalization capability in the real-world domain. We carry out an extensive evaluation on two popular datasets for depth estimation, KITTI and Make3D. The results demonstrate that our proposed method outperforms the state-of-the-art on all metrics, eg by 14.7% on Sq Rel on KITTI. The source code and model weights will be made available.",
        "pdf": "https://openaccess.thecvf.com/content/WACV2022/papers/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.pdf",
        "authors": ["HAkada", "SBhat", "IAlhashim", "PWonka"],
        "image": "/files/hiro_ss.png",
        "code": "https://github.com/hiroyasuakada/Self-Supervised-Learning-of-Domain-Invariant-Features-for-Depth-Estimation"
    },
    {
        "title": "Sketchgen: Generating constrained cad sketches",
        "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
        "date": "2021",
        "abstract": "Computer-aided design (CAD) is the most widely used modeling approach for technical design. The typical starting point in these designs is 2D sketches which can later be extruded and combined to obtain complex three-dimensional assemblies. Such sketches are typically composed of parametric primitives, such as points, lines, and circular arcs, augmented with geometric constraints linking the primitives, such as coincidence, parallelism, or orthogonality. Sketches can be represented as graphs, with the primitives as nodes and the constraints as edges. Training a model to automatically generate CAD sketches can enable several novel workflows, but is challenging due to the complexity of the graphs and the heterogeneity of the primitives and constraints. In particular, each type of primitive and constraint may require a record of different size and parameter types. We propose SketchGen as a generative model based on a transformer architecture to address the heterogeneity problem by carefully designing a sequential language for the primitives and constraints that allows distinguishing between different primitive or constraint types and their parameters, while encouraging our model to re-use information across related parameters, encoding shared structure. A particular highlight of our work is the ability to produce primitives linked via constraints that enables the final output to be further regularized via a constraint solver. We evaluate our model by demonstrating constraint prediction for given sets of primitives and full sketch generation from scratch, showing that our approach significantly out performs the state-of-the-art in CAD sketch generation.",
        "pdf": "https://proceedings.neurips.cc/paper/2021/file/28891cb4ab421830acc36b1f5fd6c91e-Paper.pdf",
        "authors": ["WPara", "SBhat", "PGuerrero", "TKelly","NMitra", "LGuibas", "PWonka"],
        "image": "/files/sketchgen.png"
    }
]